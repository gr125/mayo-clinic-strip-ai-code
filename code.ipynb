{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mayo Clinic - STRIP AI Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition link: https://www.kaggle.com/competitions/mayo-clinic-strip-ai/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of the Competition\n",
    "\n",
    "The goal of this competition is to classify the blood clot origins in ischemic stroke. Using whole slide digital pathology images, you'll build a model that differentiates between the two major acute ischemic stroke (AIS) etiology subtypes: cardiac and large artery atherosclerosis.\n",
    "\n",
    "Your work will enable healthcare providers to better identify the origins of blood clots in deadly strokes, making it easier for physicians to prescribe the best post-stroke therapeutic management and reducing the likelihood of a second stroke.\n",
    "\n",
    "### Context\n",
    "\n",
    "Stroke remains the second-leading cause of death worldwide. Each year in the United States, over 700,000 individuals experience an ischemic stroke caused by a blood clot blocking an artery to the brain. A second stroke (23% of total events are recurrent) worsens the chances of the patientâ€™s survival. However, subsequent strokes may be mitigated if physicians can determine stroke etiology, which influences the therapeutic management following stroke events.\n",
    "\n",
    "During the last decade, mechanical thrombectomy has become the standard of care treatment for acute ischemic stroke from large vessel occlusion. As a result, retrieved clots became amenable to analysis. Healthcare professionals are currently attempting to apply deep learning-based methods to predict ischemic stroke etiology and clot origin. However, unique data formats, image file sizes, as well as the number of available pathology slides create challenges you could lend a hand in solving.\n",
    "\n",
    "The Mayo Clinic is a nonprofit American academic medical center focused on integrated health care, education, and research. Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization.\n",
    "\n",
    "To decrease the chances of subsequent strokes, the Mayo Clinic Neurovascular Research Laboratory encourages data scientists to improve artificial intelligence-based etiology classification so that physicians are better equipped to prescribe the correct treatment. New computational and artificial intelligence approaches could help save the lives of stroke survivors and help us better understand the world's second-leading cause of death.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this competition comprises over a thousand high-resolution whole-slide digital pathology images. Each slide depicts a blood clot from a patient that had experienced an acute ischemic stroke.\n",
    "\n",
    "The slides comprising the training and test sets depict clots with an etiology (that is, origin) known to be either CE (Cardioembolic) or LAA (Large Artery Atherosclerosis). We include a set of supplemental slides with a either an unknown etiology or an etiology other than CE or LAA.\n",
    "\n",
    "Your task is to classify the etiology (CE or LAA) of the slides in the test set for each patient.\n",
    "\n",
    "### File and Data Field Descriptions\n",
    "* **train/** - A folder containing images in the TIFF format to be used as training data.\n",
    "* **test/** - A folder containing images to be used as test data. The actual test data comprises about 280 images.\n",
    "* **other/** - A supplemental set of images with a either an unknown etiology or an etiology other than CE or LAA.\n",
    "* **train.csv** Contains annotations for images in the train/ folder.\n",
    "    * **image_id** - A unique identifier for this instance having the form {patient_id}_{image_num}. Corresponds to the image {image_id}.tif.\n",
    "    * **center_id** - Identifies the medical center where the slide was obtained.\n",
    "    * **patient_id** - Identifies the patient from whom the slide was obtained.\n",
    "    * **image_num** - Enumerates images of clots obtained from the same patient.\n",
    "    * **label** - The etiology of the clot, either CE or LAA. This field is the classification target.\n",
    "* **test.csv** - Annotations for images in the test/ folder. Has the same fields as train.csv excluding label.\n",
    "* **other.csv** - Annotations for images in the other/ folder. Has the same fields as train.csv. The center_id is unavailable for these images however.\n",
    "    * **label** - The etiology of the clot, either Unknown or Other.\n",
    "    * **other_specified** - The specific etiology, when known, in case the etiology is labeled as Other.\n",
    "* **sample_submission.csv** - A sample submission file in the correct format. See the Evaluation page for more details. Note in particular that you should make one prediction per patient_id, not per image_id.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated using a weighted multi-class logarithmic loss. The overall effect is such that each class is roughly equally important for the final score.\n",
    "\n",
    "Each image has been labeled with an etiology class, either CE or LAA. For each image, you must submit a probability for each class. The formula is then:\n",
    "\n",
    "$$\\text{Log Loss} = - \\left( \\frac{\\sum^{M}_{i=1} w_{i} \\cdot \\sum_{j=1}^{N_{i}} \\frac{y_{ij}}{N_{i}} \\cdot \\ln  p_{ij} }{\\sum^{M}_{i=1} w_{i}} \\right)$$\n",
    "\n",
    "where $N$ is the number of images in the class set, $M$ is the number of classes, ln is the natural logarithm, $y_{ij}$ is 1 if observation $i$ belongs to class $j$ and 0 otherwise, $p_{ij}$ is the predicted probability that image $i$ belongs to $j$ class .\n",
    "\n",
    "The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, each predicted probability  is replaced with $\\max(\\min(p,1-10^{-15}),10^{-15})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "* July 6, 2022 - Start Date.\n",
    "* September 28, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete.\n",
    "* September 28, 2022 - Team Merger Deadline. This is the last day participants may join or merge teams.\n",
    "* October 5, 2022 - Final Submission Deadline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install skimage\n",
    "%pip install cv2\n",
    "%pip install pyvips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "import pyvips\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file with image paths, labels\n",
    "trainc = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\n",
    "# Convert text labels to binary labels \n",
    "trainc['label'] = [0 if i == 'CE' else 1 for i in trainc['label']]\n",
    "# Remove extremely large files to prevent memory issues\n",
    "trainc = trainc[trainc['image_id'].isin([x for x in trainc['image_id'] if os.path.getsize(f\"../input/mayo-clinic-strip-ai/train/{x}.tif\")/1000000000<1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test datsets\n",
    "trainc = trainc.sample(frac=1, random_state=12)\n",
    "indices_or_sections = [int(0.8 * len(trainc)), int((1 - 0.1 - 0.1) * len(trainc))]\n",
    "train_ds, val_ds, test_ds = np.split(trainc, indices_or_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom dataset that breaks up large image into tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, list_IDs, labels, dire=\"../input/mayo-clinic-strip-ai/train/\", dim=(5, 175, 175, 3)):\n",
    "        '''Initializes dataset.'''\n",
    "        self.root = dire\n",
    "        self.file_list = list(list_IDs)\n",
    "        if type(labels)==type(None):\n",
    "            self.labels = None\n",
    "        else:\n",
    "            self.labels = np.asarray(labels)\n",
    "        self.dim = dim\n",
    "    def __len__(self):\n",
    "        '''Returns length of dataset.'''\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns tensor of image tiles and labels if available.'''\n",
    "        img_path= self.file_list[idx]\n",
    "        if type(self.labels)!=type(None):\n",
    "            label = self.labels[idx]\n",
    "        img = self.__load_make_tiles(f\"{self.root}{img_path}.tif\" ,self.dim[1], self.dim[0])\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_tensor = img_tensor.permute(0, 3, 1, 2)\n",
    "        if type(self.labels)!=type(None):\n",
    "            label = torch.tensor([label])\n",
    "            return img_tensor, label\n",
    "        else: \n",
    "            return img_tensor\n",
    "    def __get_bg(self, img):\n",
    "        # cred: https://github.com/libvips/libvips/issues/790#issuecomment-340568993\n",
    "        # the margin of pixel we extract to get the average edge\n",
    "        margin = 10 \n",
    "\n",
    "        # paste black over the centre, take the histogram of the whole image\n",
    "        square = pyvips.Image.black(img.width - 2 * margin, img.height - 2 * margin)\n",
    "        hist = img.insert(square, margin, margin).hist_find()\n",
    "\n",
    "        # zap the 0 column to remove the black square\n",
    "        onepx = pyvips.Image.black(1, 1)\n",
    "        hist = hist.insert(onepx, 0, 0) \n",
    "\n",
    "        # then the histogram peak is the most common value in each band\n",
    "        bg = [x.maxpos()[1] for x in hist.gaussblur(1).bandsplit()]\n",
    "        \n",
    "        return bg \n",
    "\n",
    "    def __add_bg(self, img, embed=False, size=600):\n",
    "        # cred: https://github.com/libvips/libvips/issues/790#issuecomment-340568993\n",
    "        bg = self.__get_bg(img)\n",
    "        # extend image out with that background\n",
    "        img = img.embed((size - img.width) / 2, (size - img.height) / 2, size, size,\n",
    "                  extend='background', background=bg)\n",
    "        return img, bg\n",
    "    def __load_make_tiles(self, img_path, tile_size, num_tiles):\n",
    "        '''\n",
    "        img: np.ndarray with dtype np.uint8 and shape (width, height, channel)\n",
    "        '''\n",
    "        # inspired by: https://www.kaggle.com/code/analokamus/a-fast-tile-generation \n",
    "        img = pyvips.Image.new_from_file(img_path).resize(1/16)\n",
    "        patch_size = 200\n",
    "        n_across = img.width // patch_size\n",
    "        n_down = img.height // patch_size\n",
    "        x_max = n_across - 1\n",
    "        y_max = n_down - 1\n",
    "        if n_across*n_down <5:\n",
    "            if max(img.width, img.height) > 600:\n",
    "                img, bg = self.__add_bg(img, max(img.width, img.height))\n",
    "            else:\n",
    "                img, bg = self.__add_bg(img)\n",
    "            n_across = img.width // patch_size\n",
    "            n_down = img.height // patch_size\n",
    "            x_max = n_across - 1\n",
    "            y_max = n_down - 1\n",
    "        else:\n",
    "            bg = self.__get_bg(img)\n",
    "        n_patches = 0\n",
    "        patches = []\n",
    "        notpatches = []\n",
    "        for y in range(0, n_down):\n",
    "            for x in range(0, n_across):\n",
    "                patch = img.crop(x * patch_size, y * patch_size,\n",
    "                                   patch_size, patch_size)\n",
    "                for i in range(patch_size):\n",
    "                    if sum(patch.getpoint(i,i))/3<sum(bg)/3:\n",
    "                        n_patches += 1\n",
    "                        patches.append(patch)\n",
    "                        break\n",
    "                    else:\n",
    "                        notpatches.append(patch)\n",
    "        if len(patches)>=5:\n",
    "            patches = [x.numpy() for x in random.sample(patches, 5)]\n",
    "        else:\n",
    "            for j in range(5-len(patches)):\n",
    "                patches.append(random.choice(notpatches))\n",
    "            patches = [x.numpy() for x in patches]\n",
    "        img = np.moveaxis(np.stack(patches, axis=3), -1, 0)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomDataset(train_ds['image_id'], train_ds['label'])\n",
    "train_data_loader = DataLoader(train, batch_size=4, shuffle=True)\n",
    "val = CustomDataset(test_ds['image_id'], test_ds['label'])\n",
    "val_data_loader = DataLoader(val, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-summary\n",
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from timm import create_model\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x): \n",
    "        input_shape = x.shape\n",
    "        output_shape = [input_shape[i] for i in range(self.dim)] + [-1]\n",
    "        return x.view(*output_shape)\n",
    "class SimpleMIL(nn.Module):\n",
    "    # inspired by: https://www.kaggle.com/code/analokamus/a-sample-of-multi-instance-learning-model \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        num_instances=5, \n",
    "        num_classes=2, \n",
    "        pretrained=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_instances = num_instances\n",
    "        self.encoder = create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=num_classes)\n",
    "        enc_type = self.encoder.__class__.__name__\n",
    "        feature_dim = self.encoder.get_classifier().in_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveMaxPool2d(1), Flatten(),\n",
    "            nn.Linear(feature_dim, 256), nn.ReLU(inplace=True), \n",
    "            nn.Linear(256, num_classes), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: bs x N x C x W x W\n",
    "        x=x.float()\n",
    "        bs, _, ch, w, h = x.shape\n",
    "        x = x.view(bs*self.num_instances, ch, w, h) # x: N bs x C x W x W\n",
    "        x = self.encoder.forward_features(x) # x: N bs x C' x W' x W'\n",
    "\n",
    "        # Concat and pool\n",
    "        bs2, ch2, w2, h2 = x.shape\n",
    "        x = x.view(-1, self.num_instances, ch2, w2, h2).permute(0, 2, 1, 3, 4)\\\n",
    "            .contiguous().view(bs, ch2, self.num_instances*w2, h2) # x: bs x C' x N W'' x W''\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../input/strip-model-train/model.pt')\n",
    "model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by: https://www.kaggle.com/competitions/mayo-clinic-strip-ai/discussion/353305\n",
    "class WeightedLogLoss(nn.Module):\n",
    "    def __init__(self, weights=torch.tensor([0.5,0.5])):\n",
    "        super(WeightedLogLoss, self).__init__();\n",
    "        self.weights=weights\n",
    "    \n",
    "    def forward(self, probs, target):\n",
    "        log_probs = torch.log(probs)\n",
    "        res = 0\n",
    "        for c in torch.unique_consecutive(target):\n",
    "            class_log_probs = log_probs[target == c][:, c]\n",
    "            class_weight = self.weights[c]\n",
    "            res += class_weight * class_log_probs.mean()\n",
    "        return torch.tensor(- (res / torch.sum(self.weights)))\n",
    "    \n",
    "    def __call__(self, pred, target):\n",
    "        loss = self.forward(pred, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = WeightedLogLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)  \n",
    "epochs=1\n",
    "final_losses=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    i= i+1\n",
    "    loss = 0\n",
    "    for j, data in enumerate(train_data_loader): \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs, labels = data\n",
    "        labels = np.reshape(labels, (labels.shape[0], labels.shape[-1])).to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        y_pred = model(inputs)\n",
    "        bloss = loss_func(y_pred, labels.squeeze(1))\n",
    "        bloss.requires_grad = True\n",
    "        bloss.backward()\n",
    "        optimizer.step()\n",
    "        loss += bloss.item() \n",
    "    final_losses.append(loss/1000)\n",
    "    print(\"Epoch number: {} and the loss : {}\".format(i,loss/1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subc = pd.read_csv(\"../input/mayo-clinic-strip-ai/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = CustomDataset(subc['image_id'], None, dire=\"../input/mayo-clinic-strip-ai/test/\")\n",
    "sub_data_loader = DataLoader(sub, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ce = []\n",
    "laa = []\n",
    "with torch.no_grad():\n",
    "    for j, data in enumerate(sub_data_loader):\n",
    "        inputs = data.to(device)\n",
    "        predb= model(inputs)\n",
    "        for i in predb:\n",
    "            if i[0] < pow(10,-6) or i[0] >= 0.9999994: \n",
    "                ce.append(pow(10,-15)) if i[0] < pow(10,-6) else ce.append(0.999999)\n",
    "            else:\n",
    "                ce.append(i[0])\n",
    "            if i[1] < pow(10,-6) or i[1] >= 0.9999994: \n",
    "                laa.append(pow(10,-15)) if i[1] < pow(10,-6) else laa.append(0.999999)\n",
    "            else:\n",
    "                laa.append(i[1])\n",
    "\n",
    "sub_df = pd.DataFrame(list(zip(subc['patient_id'], ce, laa)), columns=['patient_id', 'CE', 'LAA'])\n",
    "sub_df = sub_df.groupby(\"patient_id\").mean()\n",
    "assert len(sub_df) == len(set(subc['patient_id'])), f\"# of rows ({len(sub_df)}) != # of patient ids ({len(set(subc['patient_id']))})\"\n",
    "sub_df = sub_df[[\"CE\", \"LAA\"]].round(6)#.set_index(\"patient_id\")\n",
    "sub_df.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Public score (loss): **0.68415**\n",
    "* **53rd** out of **888**\n",
    "* **Top 6%** \n",
    "* **Bronze** Medal (top 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1525d18273a9a114a8e240711febee22be3cdb2d9cb80941bdb3cf20b7456c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
